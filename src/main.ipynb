{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import difflib\n",
    "\n",
    "dataDir = Path('../dataset/images') \n",
    "annotationsDir = Path('../dataset/annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating expected dataframe\n",
    "\n",
    "def filelist(root, file_type):\n",
    "    return [os.path.join(directory_path, f) for directory_path, directory_name, \n",
    "            files in os.walk(root) for f in files if f.endswith(file_type)]\n",
    "\n",
    "def generate_train_multiple (anno_path):\n",
    "    annotations = filelist(anno_path, '.xml')\n",
    "    anno_list = []\n",
    "    for anno_path in annotations:\n",
    "        root = ET.parse(anno_path).getroot()\n",
    "        anno = {}\n",
    "        anno['filename'] = str(dataDir) + '/'+ root.find(\"./filename\").text\n",
    "        classArray = []\n",
    "        for child in root:\n",
    "            if child.tag == \"object\":\n",
    "                for grandchild in child:\n",
    "                    if grandchild.tag == \"name\":\n",
    "                        if grandchild.text != \"trafficlight\":\n",
    "                            classArray.append(grandchild.text)\n",
    "        anno['expected'] = classArray\n",
    "        if len(classArray) != 0:\n",
    "            anno_list.append(anno)\n",
    "            \n",
    "    return pd.DataFrame(anno_list)\n",
    "\n",
    "def generate_train_single (anno_path):\n",
    "    annotations = filelist(anno_path, '.xml')\n",
    "    anno_list = []\n",
    "    for anno_path in annotations:\n",
    "        root = ET.parse(anno_path).getroot()\n",
    "        anno = {}\n",
    "        anno['filename'] = str(dataDir) + '/'+ root.find(\"./filename\").text\n",
    "        classArray = []\n",
    "        for child in root:\n",
    "            if child.tag == \"object\":\n",
    "                for grandchild in child:\n",
    "                    if grandchild.tag == \"name\":\n",
    "                        if grandchild.text != \"trafficlight\":\n",
    "                            classArray.append(grandchild.text)\n",
    "        anno['expected'] = classArray\n",
    "        if len(classArray) == 1:\n",
    "            anno_list.append(anno)\n",
    "            \n",
    "    return pd.DataFrame(anno_list)\n",
    "\n",
    "def generate_train_simple (anno_path):\n",
    "    annotations = filelist(anno_path, '.xml')\n",
    "    anno_list = []\n",
    "    for anno_path in annotations:\n",
    "        root = ET.parse(anno_path).getroot()\n",
    "        anno = {}\n",
    "        anno['filename'] = str(dataDir) + '/'+ root.find(\"./filename\").text\n",
    "        for child in root:\n",
    "            if child.tag == \"object\":\n",
    "                for grandchild in child:\n",
    "                    if grandchild.tag == \"name\":\n",
    "                        anno['expected'] = grandchild.text\n",
    "        if int(anno['filename'][22:len(anno['filename'])-4]) < 170:\n",
    "            anno_list.append(anno)\n",
    "        \n",
    "    return pd.DataFrame(anno_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Pre-processing\n",
    "\n",
    "# Improve Lighting\n",
    "\n",
    "def improve_lighting(img):\n",
    "    imgYUV = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    imgYUV[:, :, 0] = cv2.equalizeHist(imgYUV[:, :, 0])\n",
    "\n",
    "    imgBetterLighting = cv2.cvtColor(imgYUV, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "    imgHSV = cv2.cvtColor(imgBetterLighting, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(imgHSV)\n",
    "\n",
    "    lim = 255 - 50\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += 50\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    imgBetterLighting = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return imgBetterLighting\n",
    "\n",
    "# Smooth\n",
    "\n",
    "def image_smooth(img):\n",
    "    imgWithMedianFilter = cv2.medianBlur(img, 5)\n",
    "\n",
    "    return imgWithMedianFilter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Segmentation\n",
    "\n",
    "def image_segmentation(img):\n",
    "    #set the bounds for the red hue\n",
    "    lower_red_n1 = np.array([0,70,60])\n",
    "    upper_red_n1 = np.array([10,255,255])\n",
    "\n",
    "    lower_red_n2 = np.array([170,70,60])\n",
    "    upper_red_n2 = np.array([180,255,255])\n",
    "\n",
    "    lower_blue_n3 = np.array([78,158,124])\n",
    "    upper_blue_n3 = np.array([138,255,255])\n",
    "\n",
    "    #create a mask using the bounds set\n",
    "\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "    mask_1 = cv2.inRange(img_hsv, lower_red_n1, upper_red_n1)\n",
    "    mask_2 = cv2.inRange(img_hsv, lower_red_n2, upper_red_n2)\n",
    "\n",
    "    mask_red = mask_1 + mask_2\n",
    "\n",
    "    mask_blue = cv2.inRange(img_hsv, lower_blue_n3, upper_blue_n3)\n",
    "\n",
    "    return mask_blue, mask_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Thresholding and Morphological Operations\n",
    "\n",
    "def morphological_ops(img):\n",
    "    # Red\n",
    "\n",
    "    # Removing Noise\n",
    "    kernel = np.ones((3, 3),np.uint8)\n",
    "    processed_red = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iterations = 1)\n",
    "    processed_red = cv2.morphologyEx(processed_red, cv2.MORPH_DILATE, kernel, iterations = 1)\n",
    "\n",
    "    # Floodfill\n",
    "\n",
    "    red_floodfill = processed_red.copy()\n",
    "\n",
    "    h, w = processed_red.shape[:2]\n",
    "\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "\n",
    "    cv2.floodFill(red_floodfill, mask, (0,0), 255)\n",
    "\n",
    "    red_floodfill_inv = cv2.bitwise_not(red_floodfill)\n",
    "\n",
    "    filled_image = processed_red | red_floodfill_inv\n",
    "\n",
    "    return filled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape Recognition\n",
    "\n",
    "def shape_recognition(img_red, img_blue, initial_image):\n",
    "    \n",
    "    result = \"\"\n",
    "    results = []\n",
    "    img_red_contours = initial_image\n",
    "    img_blue_contours = initial_image\n",
    "\n",
    "    # Red\n",
    "    debug = \"red\"\n",
    "\n",
    "    # Octagon Detection\n",
    "    contours_red, hierarchy_red = cv2.findContours(img_red, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    maxArea = 0\n",
    "\n",
    "    for cnt in contours_red:\n",
    "        approx = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt, True), True)\n",
    "        #print(len(approx))\n",
    "\n",
    "        if len(approx) == 8:\n",
    "            cntArea = cv2.contourArea(cnt)\n",
    "            if cntArea > maxArea:\n",
    "                maxArea = cntArea\n",
    "                maxContour = cnt\n",
    "                result = \"stop\"\n",
    "\n",
    "            # print('Found STOP sign')\n",
    "            results.append(\"stop\")\n",
    "            img_red_contours = cv2.drawContours(initial_image, [cnt], 0, (0,0,255), -1)\n",
    "\n",
    "        elif len(approx) >= 12:\n",
    "            cntArea = cv2.contourArea(cnt)\n",
    "            if cntArea > maxArea:\n",
    "                maxArea = cntArea\n",
    "                maxContour = cnt\n",
    "                result = \"speedlimit\"\n",
    "\n",
    "            # print('Found red circle sign')           \n",
    "            results.append(\"speedlimit\")\n",
    "            img_red_contours = cv2.drawContours(initial_image, [cnt], 0, (0,0,255), -1)\n",
    "\n",
    "    # Blue\n",
    "\n",
    "    contours_blue, hierarchy_blue = cv2.findContours(img_blue, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for cnt in contours_blue:\n",
    "        approx = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt, True), True)\n",
    "        # print(len(approx))\n",
    "\n",
    "        if len(approx) == 4:\n",
    "            cntArea = cv2.contourArea(cnt)\n",
    "            if cntArea > maxArea:\n",
    "                maxArea = cntArea\n",
    "                maxContour = cnt\n",
    "                result = \"crosswalk\"\n",
    "        \n",
    "            # print('Found BLUE sign')\n",
    "            results.append(\"crosswalk\")\n",
    "            # debug = \"blue\"\n",
    "            img_blue_contours = cv2.drawContours(initial_image, [maxContour], 0, (255,0,0), -1)\n",
    "\n",
    "    \"\"\"\n",
    "    if debug==\"red\":\n",
    "        plt.imshow(cv2.cvtColor(img_red_contours, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Foreground')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    elif debug == \"blue\":\n",
    "        plt.imshow(cv2.cvtColor(img_blue_contours, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Foreground')\n",
    "        plt.axis('off')\n",
    "        plt.show() \n",
    "    \"\"\"\n",
    "    \n",
    "    return result, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image(imgPath):\n",
    "    \n",
    "    img = cv2.imread(imgPath)\n",
    "\n",
    "    imgLighting = improve_lighting(img)\n",
    "    imgSmooth = image_smooth(imgLighting)\n",
    "    img_blue, img_red = image_segmentation(imgSmooth)\n",
    "    processed_blue = morphological_ops(img_blue)\n",
    "    processed_red = morphological_ops(img_red)\n",
    "    result = shape_recognition(processed_red, processed_blue, img)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating prediction Dataframe\n",
    "\n",
    "def generate_prediction(df_compare):\n",
    "    pred_list = []\n",
    "    for index, row in df_compare.iterrows():\n",
    "        pred = {}\n",
    "        pred[\"filename\"] = row[\"filename\"]\n",
    "        result, results = evaluate_image(row[\"filename\"])\n",
    "        pred[\"prediction\"] = results\n",
    "        pred_list.append(pred)\n",
    "    return pd.DataFrame(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging dataframes\n",
    "\n",
    "def join_dataframes(df_train, df_pred):\n",
    "    df_merged = pd.merge(df_train, df_pred, on='filename')\n",
    "\n",
    "    for index, row in df_merged.iterrows():\n",
    "        sm = difflib.SequenceMatcher(None,row[\"expected\"],row[\"prediction\"])\n",
    "        df_merged.at[index,\"Similarity\"] = sm.ratio()\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.281366565401477\n"
     ]
    }
   ],
   "source": [
    "# Multiple sign Classification\n",
    "df_compare = generate_train_multiple(annotationsDir)\n",
    "df_pred = generate_prediction(df_compare)\n",
    "\n",
    "df_multiple = generate_train_multiple(annotationsDir)\n",
    "df_merged = join_dataframes(df_multiple, df_pred)\n",
    "df_merged.head()\n",
    "print(df_merged[\"Similarity\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27418667204263203\n"
     ]
    }
   ],
   "source": [
    "# One sign Classification\n",
    "df_compare = generate_train_single(annotationsDir)\n",
    "df_pred = generate_prediction(df_compare)\n",
    "\n",
    "df_single = generate_train_single(annotationsDir)\n",
    "df_merged = join_dataframes(df_single, df_pred)\n",
    "df_merged.head()\n",
    "print(df_merged[\"Similarity\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# First 170 images Classification\n",
    "df_compare = generate_train_simple(annotationsDir)\n",
    "df_pred = generate_prediction(df_compare)\n",
    "\n",
    "df_simple = generate_train_simple(annotationsDir)\n",
    "df_merged = join_dataframes(df_simple, df_pred)\n",
    "df_merged.head()\n",
    "print(df_merged[\"Similarity\"].mean())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
